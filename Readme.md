
```md
# üìÑ AI Search Chat (Perplexity-style RAG System)

A full-stack AI application that allows users to **upload PDFs**, ask natural language questions, and receive **streamed AI responses with inline citations**. Clicking on a citation opens a **PDF viewer** that automatically navigates to and highlights the referenced section.

The system is inspired by **Perplexity AI** and demonstrates **Retrieval-Augmented Generation (RAG)** with a **Generative UI**.

---

## ‚ú® Key Features

- üìÑ Upload PDFs dynamically from the frontend
- üîç Semantic search over PDF content (RAG)
- üí¨ Perplexity-style chat interface
- ‚ö° Real-time streaming responses using SSE
- üß† Tool call indicators (e.g. *Searching documents‚Ä¶*)
- üî¢ Inline numbered citations `[1] [2] [3]`
- üìö Source cards below responses
- üìñ PDF Viewer with:
  - Smooth slide-in animation
  - Auto-navigation to cited page
  - Highlighted / underlined cited text
  - Zoom & page navigation
- ‚ôªÔ∏è Session reset (clear PDFs + vector store)

---

## üß± Tech Stack

### Frontend
- **Next.js (App Router)**
- **TypeScript**
- **Tailwind CSS**
- **Framer Motion** (animations)
- **react-pdf / pdf.js**
- **Lucide Icons**

### Backend
- **FastAPI**
- **Python 3.11**
- **Server-Sent Events (SSE)**
- **SentenceTransformers** (local embeddings)
- **FAISS** (vector similarity search)
- **pdfplumber** (PDF parsing)
- **Ollama** (local LLM inference)

---

## üèó Architecture Overview

```

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend    ‚îÇ  (Next.js)
‚îÇ              ‚îÇ
‚îÇ  Chat UI     ‚îÇ‚îÄ‚îÄ‚ñ∂ /chat/stream (SSE)
‚îÇ  PDF Upload  ‚îÇ‚îÄ‚îÄ‚ñ∂ /upload/pdf
‚îÇ  PDF Viewer  ‚îÇ‚îÄ‚îÄ‚ñ∂ /pdf/{filename}
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ
‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend     ‚îÇ  (FastAPI)
‚îÇ              ‚îÇ
‚îÇ  PDF Ingest  ‚îÇ‚îÄ‚îÄ‚ñ∂ Chunking
‚îÇ  Embeddings  ‚îÇ‚îÄ‚îÄ‚ñ∂ FAISS Vector Store
‚îÇ  Retriever  ‚îÇ‚îÄ‚îÄ‚ñ∂ Top-K Context
‚îÇ  LLM Stream ‚îÇ‚îÄ‚îÄ‚ñ∂ Token Streaming
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

````

---

## üîÑ Streaming Protocol (SSE)

The backend streams multiple event types over a single SSE connection:

| Event | Purpose |
|-----|--------|
| `tool` | Tool / reasoning updates |
| `text` | Incremental LLM output |
| `citation` | Document + page + snippet |
| `done` | End of response |

This enables:
- Real-time typing effect
- Tool call UI indicators
- Progressive UI updates

---

## üß† RAG Pipeline

1. User uploads PDF from UI
2. PDF stored in `backend/pdfs/`
3. Text extracted using `pdfplumber`
4. Text split into overlapping chunks
5. Chunks embedded using **SentenceTransformer (local)**
6. Stored in FAISS vector index
7. User query embedded
8. Top-K chunks retrieved
9. LLM answers **only using retrieved context**
10. Citations streamed alongside response

---

## ‚öôÔ∏è Setup Instructions

### 1Ô∏è‚É£ Backend Setup

```bash
cd backend
python -m venv venv
source venv/bin/activate  # macOS / Linux
# venv\Scripts\activate   # Windows

pip install -r requirements.txt
````

Run backend:

```bash
uvicorn main:app --reload --port 8000
```

Backend URL:

```
http://localhost:8000
```

---

### 2Ô∏è‚É£ Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Frontend URL:

```
http://localhost:3000
```

---

## üå± Environment Variables

### Backend (`backend/.env`)

```env
OLLAMA_BASE_URL=http://localhost:11434
```

> No API keys are required ‚Äî the system runs fully locally.

---

## üñº Screenshots / GIFs

Create a folder:

```
/screenshots
```

Add the following files:

### 1Ô∏è‚É£ Tool Call Streaming

```
screenshots/tool-streaming.gif
```

Shows:

* ‚ÄúSearching documents‚Ä¶‚Äù
* Live token streaming

### 2Ô∏è‚É£ Generative UI Rendering

```
screenshots/generative-ui.png
```

Shows:

* Inline citations
* Source cards
* Clean chat UI

### 3Ô∏è‚É£ Citation ‚Üí PDF Viewer Transition

```
screenshots/citation-to-pdf.gif
```

Shows:

* Clicking `[1]`
* PDF viewer sliding in
* Highlighted text

---

## üì¶ Libraries Used & Justification

| Library              | Reason                  |
| -------------------- | ----------------------- |
| FastAPI              | Async backend + SSE     |
| sse-starlette        | SSE support             |
| SentenceTransformers | Local embeddings        |
| FAISS                | Fast vector similarity  |
| pdfplumber           | Reliable PDF extraction |
| Ollama               | Local LLM inference     |
| Next.js              | Modern React framework  |
| Framer Motion        | Smooth transitions      |
| react-pdf            | PDF rendering           |

---

## üß© Design Decisions

### Why SSE over WebSockets?

* Simpler protocol
* One-way streaming suits LLM output
* Native browser support

### Why local embeddings & LLM?

* No API cost
* Offline-friendly
* Deterministic behavior

### Why in-memory vector store?

* Faster iteration
* Simpler system for assignment scope

---

## ‚ö†Ô∏è Trade-offs & Limitations

* Vector store resets on server restart
* No persistent database
* Single-user session
* PDF highlighting is text-layer based (best-effort)

---

## ‚ôªÔ∏è Reset Session

The backend supports resetting the session:

* Deletes uploaded PDFs
* Clears chunks
* Reinitializes vector store

Useful for starting fresh without restarting the server.

---

## üöÄ Future Improvements

* Persistent FAISS index
* Multi-document filtering
* User authentication
* Charts & tables as generative UI
* Hybrid search (BM25 + embeddings)

---
